# SilentCue - Real-Time Sign Language Translator for Deaf and Non-Verbal Individuals

SilentCue is an AI-driven application designed to bridge the communication gap for deaf and non-verbal individuals. It enables real-time American Sign Language (ASL) gesture recognition using computer vision and machine learning, allowing users to form letters, words, and sentences seamlessly. The system also offers gesture training, dynamic word suggestions, system control, and video call integration, enhancing communication and accessibility.

---

## Features
- âœ‹ **Real-Time Hand Gesture Detection** using OpenCV and MediaPipe.
- ğŸ§  **Intelligent Word Suggestions** via NLTK to form meaningful sentences.
- âŒ¨ï¸ **Letter Case Toggle** (Uppercase/Lowercase) for better sentence construction.
- ğŸ§© **Custom Gesture Training** to add personalized signs.
- ğŸ“ˆ **Dynamic Sentence Formation** and Editing Options.
- ğŸ–¥ï¸ **Interactive Tkinter GUI** for an easy-to-use interface.
- ğŸ”— **Video Call Integration** (TokBox) for remote communication.
- ğŸ¯ **System Action Control** (e.g., opening applications, basic automation).
- ğŸ“– **Gesture Guide Access** for learning standard gestures.

---

## Tech Stack
- **Programming Language**: Python 3

- **Libraries Used**:
  - OpenCV
  - MediaPipe
  - scikit-learn
  - TensorFlow/Keras (for training models)
  - NLTK (for word suggestion and sentence formation)
  - Tkinter (for GUI development)
    
## Dataset Usage Policy

The dataset used for training and evaluating SilentCue has been custom-created specifically for this project. It is intended solely for research, educational, and accessibility enhancement purposes. 

Users are strictly advised not to misuse, redistribute, or employ the dataset for any improper, unauthorized, or malicious activities. Any use beyond the intended purpose must be carried out with prior consent and in compliance with ethical standards and applicable laws.
